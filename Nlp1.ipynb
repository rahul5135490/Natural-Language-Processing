{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2af5bc9f-4f94-4a75-8c96-471236ec34c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello, world! This is a sample sentence for NLP preprocessing. It's quite interesting.\n",
      "Word Tokens: ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'sample', 'sentence', 'for', 'NLP', 'preprocessing', '.', 'It', \"'s\", 'quite', 'interesting', '.']\n",
      "Sentence Tokens: ['Hello, world!', 'This is a sample sentence for NLP preprocessing.', \"It's quite interesting.\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data (do this once)\n",
    "# You might get a pop-up window, select 'punkt' and 'stopwords' and click download.\n",
    "# If you don't get a pop-up, you can try:\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# If you get a \"Resource 'tokenizers/punkt' not found\" error, run the above downloads.\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello, world! This is a sample sentence for NLP preprocessing. It's quite interesting.\"\n",
    "\n",
    "# 1. Word Tokenization: Splitting text into individual words\n",
    "words = word_tokenize(text)\n",
    "print(f\"Original Text: {text}\")\n",
    "print(f\"Word Tokens: {words}\")\n",
    "\n",
    "# 2. Sentence Tokenization: Splitting text into individual sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(f\"Sentence Tokens: {sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e54af6-4cb6-466f-b4a4-2bf9c044820d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello, World! This is a Sample Sentence with Punctuation.\n",
      "Lowercased Text: hello, world! this is a sample sentence with punctuation.\n",
      "Text without punctuation (Method A): Hello World This is a Sample Sentence with Punctuation\n",
      "Text without punctuation (Method B - regex): Hello World This is a Sample Sentence with Punctuation\n",
      "Combined (Lowercase & No Punctuation): hello world this is a sample sentence with punctuation\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = \"Hello, World! This is a Sample Sentence with Punctuation.\"\n",
    "\n",
    "# 1. Lowercasing\n",
    "lower_text = text.lower()\n",
    "print(f\"Original Text: {text}\")\n",
    "print(f\"Lowercased Text: {lower_text}\")\n",
    "\n",
    "# 2. Removing Punctuation\n",
    "# Method A: Using str.translate and string.punctuation\n",
    "# This creates a translation table that maps each punctuation character to None (effectively deleting it)\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "no_punct_text_a = text.translate(translator)\n",
    "print(f\"Text without punctuation (Method A): {no_punct_text_a}\")\n",
    "\n",
    "# Method B: Using Regular Expressions (re module)\n",
    "import re\n",
    "# re.sub(pattern, replacement, string)\n",
    "# [^\\w\\s] matches any character that is NOT a word character (alphanumeric) and NOT a whitespace character\n",
    "no_punct_text_b = re.sub(r'[^\\w\\s]', '', text)\n",
    "print(f\"Text without punctuation (Method B - regex): {no_punct_text_b}\")\n",
    "\n",
    "# Combine: Lowercase and remove punctuation\n",
    "cleaned_text_combined = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "print(f\"Combined (Lowercase & No Punctuation): {cleaned_text_combined}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b648045b-627d-4111-bec1-d6f45f2e1ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 English Stop Words: ['my', \"weren't\", 'during', 'being', \"you'd\", 'once', 'an', \"couldn't\", 'yourselves', 'hasn']\n",
      "\n",
      "Original Words: ['This', 'is', 'a', 'very', 'important', 'sentence', 'demonstrating', 'stop', 'word', 'removal', '.']\n",
      "Filtered Words (No Stop Words): ['important', 'sentence', 'demonstrating', 'stop', 'word', 'removal', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sample_text = \"This is a very important sentence demonstrating stop word removal.\"\n",
    "\n",
    "# Get English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"First 10 English Stop Words: {list(stop_words)[:10]}\")\n",
    "\n",
    "# Tokenize the sample text\n",
    "words = word_tokenize(sample_text)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(f\"\\nOriginal Words: {words}\")\n",
    "print(f\"Filtered Words (No Stop Words): {filtered_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f6ff88c-c14f-4c7e-8e9b-51123ab41798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['running', 'runs', 'ran', 'generously', 'universal', 'better', 'studying', 'studies']\n",
      "Stemmed Words: ['run', 'run', 'ran', 'gener', 'univers', 'better', 'studi', 'studi']\n",
      "Lemmatized Words (default POS=noun): ['running', 'run', 'ran', 'generously', 'universal', 'better', 'studying', 'study']\n",
      "Lemmatized Words (with specified POS): ['run', 'run', 'run', 'good', 'study', 'study']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download WordNet data for lemmatization (do this once)\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4') # Open Multilingual Wordnet (often needed with wordnet)\n",
    "\n",
    "words_to_process = [\"running\", \"runs\", \"ran\", \"generously\", \"universal\", \"better\", \"studying\", \"studies\"]\n",
    "\n",
    "# 1. Stemming using Porter Stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in words_to_process]\n",
    "print(f\"Original Words: {words_to_process}\")\n",
    "print(f\"Stemmed Words: {stemmed_words}\")\n",
    "\n",
    "# 2. Lemmatization using WordNet Lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# Lemmatization often requires specifying the Part of Speech (POS) tag for better accuracy.\n",
    "# 'v' for verb, 'n' for noun, 'a' for adjective, 'r' for adverb.\n",
    "# If POS is not specified, it defaults to 'n' (noun).\n",
    "lemmatized_words_default = [wordnet_lemmatizer.lemmatize(word) for word in words_to_process]\n",
    "print(f\"Lemmatized Words (default POS=noun): {lemmatized_words_default}\")\n",
    "\n",
    "# Example with specified POS for better results for verbs/adjectives\n",
    "lemmatized_words_pos = [\n",
    "    wordnet_lemmatizer.lemmatize(\"running\", pos='v'),\n",
    "    wordnet_lemmatizer.lemmatize(\"runs\", pos='v'),\n",
    "    wordnet_lemmatizer.lemmatize(\"ran\", pos='v'),\n",
    "    wordnet_lemmatizer.lemmatize(\"better\", pos='a'),\n",
    "    wordnet_lemmatizer.lemmatize(\"studying\", pos='v'),\n",
    "    wordnet_lemmatizer.lemmatize(\"studies\", pos='v')\n",
    "]\n",
    "print(f\"Lemmatized Words (with specified POS): {lemmatized_words_pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "685c555d-450d-434c-9fad-f399ba13275e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Paragraph:\n",
      "Natural Language Processing (NLP) is a fascinating field. It combines computer science, artificial intelligence, and linguistics. Many new advancements are running quickly, making machines understand human language better.\n",
      "\n",
      "Processed Tokens:\n",
      "['natural', 'language', 'processing', 'nlp', 'fascinating', 'field', 'combine', 'computer', 'science', 'artificial', 'intelligence', 'linguistics', 'many', 'new', 'advancement', 'running', 'quickly', 'making', 'machine', 'understand', 'human', 'language', 'better']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded (if not already)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize lemmatizer and stop words outside the function for efficiency\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Applies a series of preprocessing steps to a given text.\n",
    "    - Lowercasing\n",
    "    - Punctuation removal\n",
    "    - Tokenization\n",
    "    - Stop word removal\n",
    "    - Lemmatization (defaults to noun if POS not determined)\n",
    "    \"\"\"\n",
    "    # 1. Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Punctuation Removal (using regex, similar to Method B from earlier)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Keep only word characters (alphanumeric) and spaces\n",
    "\n",
    "    # 3. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 4. Stop Word Removal\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # 5. Lemmatization\n",
    "    # For better lemmatization, one would typically use POS tagging,\n",
    "    # but for simplicity in this introductory exercise, we'll use default.\n",
    "    # You can add logic here to determine POS if you wish to explore more advanced concepts later.\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Test the complete preprocessing function\n",
    "sample_paragraph = \"Natural Language Processing (NLP) is a fascinating field. It combines computer science, artificial intelligence, and linguistics. Many new advancements are running quickly, making machines understand human language better.\"\n",
    "\n",
    "processed_paragraph_tokens = preprocess_text(sample_paragraph)\n",
    "print(f\"Original Paragraph:\\n{sample_paragraph}\")\n",
    "print(f\"\\nProcessed Tokens:\\n{processed_paragraph_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab50898-b80b-4bda-839e-69af477a5dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
