{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e6ade7-64e3-443d-855f-976f28af320d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Documents:\n",
      "Doc 1: The quick brown fox jumps over the lazy dog.\n",
      "Doc 2: Dogs are loyal animals and are good pets.\n",
      "Doc 3: Foxes are clever animals, but sometimes lazy.\n",
      "Doc 4: A quick brown dog is a good pet.\n",
      "\n",
      "Preprocessed Documents:\n",
      "Doc 1: quick brown fox jump lazy dog\n",
      "Doc 2: dog loyal animal good pet\n",
      "Doc 3: fox clever animal sometimes lazy\n",
      "Doc 4: quick brown dog good pet\n",
      "\n",
      "Vocabulary (Feature Names):\n",
      "['animal' 'brown' 'clever' 'dog' 'fox' 'good' 'jump' 'lazy' 'loyal' 'pet'\n",
      " 'quick' 'sometimes']\n",
      "\n",
      "Bag-of-Words Matrix (Dense):\n",
      "[[0 1 0 1 1 0 1 1 0 0 1 0]\n",
      " [1 0 0 1 0 1 0 0 1 1 0 0]\n",
      " [1 0 1 0 1 0 0 1 0 0 0 1]\n",
      " [0 1 0 1 0 1 0 0 0 1 1 0]]\n",
      "\n",
      "Bag-of-Words DataFrame:\n",
      "   animal  brown  clever  dog  fox  good  jump  lazy  loyal  pet  quick  \\\n",
      "0       0      1       0    1    1     0     1     1      0    0      1   \n",
      "1       1      0       0    1    0     1     0     0      1    1      0   \n",
      "2       1      0       1    0    1     0     0     1      0    0      0   \n",
      "3       0      1       0    1    0     1     0     0      0    1      1   \n",
      "\n",
      "   sometimes  \n",
      "0          0  \n",
      "1          0  \n",
      "2          1  \n",
      "3          0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "# --- Preprocessing Function (from Day 8) ---\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    return \" \".join(lemmatized_tokens) # Join back into a string for CountVectorizer\n",
    "\n",
    "# Sample documents (sentences)\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Dogs are loyal animals and are good pets.\",\n",
    "    \"Foxes are clever animals, but sometimes lazy.\",\n",
    "    \"A quick brown dog is a good pet.\"\n",
    "]\n",
    "\n",
    "print(\"Original Documents:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Doc {i+1}: {doc}\")\n",
    "\n",
    "# Apply preprocessing to each document\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "print(\"\\nPreprocessed Documents:\")\n",
    "for i, doc in enumerate(preprocessed_documents):\n",
    "    print(f\"Doc {i+1}: {doc}\")\n",
    "\n",
    "# 1. Initialize CountVectorizer\n",
    "# min_df (minimum document frequency) ignores words that appear in too few documents (e.g., specific to only one doc)\n",
    "# max_df (maximum document frequency) ignores words that appear in too many documents (e.g., common across all docs)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# 2. Fit the vectorizer to the preprocessed documents and transform them\n",
    "# fit(): Learns the vocabulary from the documents\n",
    "# transform(): Converts the documents into numerical vectors based on the learned vocabulary\n",
    "X_bow = vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# 3. Print the vocabulary (words it learned)\n",
    "print(f\"\\nVocabulary (Feature Names):\\n{vectorizer.get_feature_names_out()}\")\n",
    "\n",
    "# 4. Print the Bag-of-Words matrix (sparse matrix, convert to dense for easy viewing)\n",
    "# Each row represents a document, each column represents a word from the vocabulary,\n",
    "# and the value is the count of that word in the document.\n",
    "print(f\"\\nBag-of-Words Matrix (Dense):\\n{X_bow.toarray()}\")\n",
    "\n",
    "# Let's see which column corresponds to which word for a clear view\n",
    "import pandas as pd\n",
    "df_bow = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(f\"\\nBag-of-Words DataFrame:\\n{df_bow}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "726b4f26-84ae-4434-afc5-b84d179796d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vocabulary (Feature Names):\n",
      "['animal' 'brown' 'clever' 'dog' 'fox' 'good' 'jump' 'lazy' 'loyal' 'pet'\n",
      " 'quick' 'sometimes']\n",
      "\n",
      "TF-IDF Matrix (Dense):\n",
      "[[0.         0.39954636 0.         0.32346721 0.39954636 0.\n",
      "  0.5067739  0.39954636 0.         0.         0.39954636 0.        ]\n",
      " [0.43584673 0.         0.         0.35285549 0.         0.43584673\n",
      "  0.         0.         0.55281632 0.43584673 0.         0.        ]\n",
      " [0.40104275 0.         0.50867187 0.         0.40104275 0.\n",
      "  0.         0.40104275 0.         0.         0.         0.50867187]\n",
      " [0.         0.46346838 0.         0.3752176  0.         0.46346838\n",
      "  0.         0.         0.         0.46346838 0.46346838 0.        ]]\n",
      "\n",
      "TF-IDF DataFrame:\n",
      "     animal     brown    clever       dog       fox      good      jump  \\\n",
      "0  0.000000  0.399546  0.000000  0.323467  0.399546  0.000000  0.506774   \n",
      "1  0.435847  0.000000  0.000000  0.352855  0.000000  0.435847  0.000000   \n",
      "2  0.401043  0.000000  0.508672  0.000000  0.401043  0.000000  0.000000   \n",
      "3  0.000000  0.463468  0.000000  0.375218  0.000000  0.463468  0.000000   \n",
      "\n",
      "       lazy     loyal       pet     quick  sometimes  \n",
      "0  0.399546  0.000000  0.000000  0.399546   0.000000  \n",
      "1  0.000000  0.552816  0.435847  0.000000   0.000000  \n",
      "2  0.401043  0.000000  0.000000  0.000000   0.508672  \n",
      "3  0.000000  0.000000  0.463468  0.463468   0.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Using the same preprocessed_documents from Assignment 1\n",
    "\n",
    "# 1. Initialize TfidfVectorizer\n",
    "# Like CountVectorizer, it learns vocabulary and transforms.\n",
    "# It also applies TF-IDF weighting.\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 2. Fit and transform the preprocessed documents\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# 3. Print the vocabulary (should be the same as BoW if using the same settings)\n",
    "print(f\"TF-IDF Vocabulary (Feature Names):\\n{tfidf_vectorizer.get_feature_names_out()}\")\n",
    "\n",
    "# 4. Print the TF-IDF matrix (sparse matrix, convert to dense for easy viewing)\n",
    "# Higher values indicate words that are more important to that specific document in the collection.\n",
    "print(f\"\\nTF-IDF Matrix (Dense):\\n{X_tfidf.toarray()}\")\n",
    "\n",
    "# Let's see which column corresponds to which word for a clear view\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(f\"\\nTF-IDF DataFrame:\\n{df_tfidf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b3fa7-bed5-4528-b237-7d61af898ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
